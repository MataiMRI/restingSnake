#  NeSI

This documentation details how to run the workflow on [NeSI](https://www.nesi.org.nz/) (New Zealand eScience Infrastructure) HPC platform.


## Getting started

First, make sure to be logged on Mahuika, either via SSH or using [Jupyter on NeSI](https//jupyter.nesi.org.nz).

In a terminal, clone this repository within your project folder:

```
cd /nesi/project/PROJECTID/$USER
git clone https://github.com/jpmcgeown/fmri_workflow.git
```

where `PROJECTID` is your project number (e.g. `uoa03264`).

*Note: you only need to do the cloning step once.*

Then change directory to be in the folder of the repository:

```
cd /nesi/project/PROJECTID/$USER/fmri_workflow
```

and edit the `config.yml` file to set your input dataset and result folder paths, for example using `nano` editor:

```
nano config.yml
```

or the JupyterLab editor if you logged in via Jupyter.

Then, instead of using `snakemake` command directly, we will use the `nesi/snakemake.sl` script, which takes care of setting up the environment to run Snakemake on NeSI.

First, always do a dry-run and see which files will be (re-)created using:

```
srun --account=PROJECTID --qos=debug nesi/snakemake.sl -n
```

*Note: The option `--qos=debug` gives a higher priority but can only be used for short-running jobs (max 15 minutes). It's ideal for dry-runs.*

If everything looks good, it is then time to submit the workflow as a Slurm batch job using the `sbatch` command:

```
sbatch --account=PROJECTID nesi/snakemake.sl
```

This puts the workflow in the Slurm queue, where is should be scheduled to start as soon as resources are available.
This command print a number, the **job ID**, that will be useful to keep track of the execution of the workflow.
Note that you don't need to stay logged in once the job as been submitted.

### Job time limit

By default, the workflow time limit is 2 days.
You can increase it up to 3 weeks by using the `--time` option of `sbatch`.

For example, to increase it to 4 days, use:

```
sbatch --account=PROJECTID --time=04-00:00 nesi/snakemake.sl
```

If you notice that your current job will run out of time, do not hesitate to send an email to support@nesi.org.nz to ask for an extension, providing the job ID of the workflow.


### Running multiple workflows

You can only run one Snakemake workflow at a time (per project and user).

If you use `sbatch` multiple times, the next execution of the workflow will only start once the current one has finished.


### Snakemake options

You can use all the usual snakemake command line options with `nesi/snakemake.sl`, by adding then at the end of your `srun` or `sbatch` command.

For example, to run the workflow until the `freesurfer` rule, use:

```
sbatch --account=PROJECTID nesi/snakemake.sl --until freesurfer
```

See the [README](README.md#Useful-Snakemake-options) document for useful Snakemake options.


## Job management

To check the status of your Slurm job, use:

- either the **Job ID** directly

  ```
  squeue -j JOBID
  ```

- or filtering your job list to find the job by its name

  ```
  squeue --me -n fmri_workflow
  ```

If the job does not appear in the list, it means that it has completed.

Use the `sacct` command to check if this has been successful or if it ran into issues:

```
sacct -j JOBID
```

If you need to cancel the workflow, use the `scancel` command as follows:

```
scancel --signal INT --full JOBID
```

*Note: The options `--signal INT` and `--full` are very important to ensure all jobs related to this workflow are properly cancelled, not only the main Snakemake job.*


## Workflow monitoring

All log files generated by the workflow are saved in the `nesi/logs` folder.

While the workflow is running, you can look at messages from Snakemake in the corresponding log file using:

```
cat nesi/logs/JOBID-fmri_workflow.out
```

or  keep a *live* view of the messages as they are appended to the file as follows:

```
tail -f nesi/logs/JOBID-fmri_workflow.out
```

and press CTRL-C to get back the command prompt.

Note that every rule also generates a Slurm log file named `nesi/logs/RULE_JOBID-RULENAME.out`.
The corresponding job ID `RULE_JOBID` is listed in the `nesi/logs/JOBID-fmri_workflow.out` log file.


## Accessing JupyterLab via SSH

JupyterLab is a convenient way to explore the results of the workflow (e.g. fmriprep html reports).

We recommend to use it via an SSH tunnel to the Mahuika login node.
If you are using a terminal, add the `-L` option to your ssh command, for example:
fmri_workflow.out
```
ssh mahuika -L PORT:localhost:PORT
```

where `PORT` is an arbitrary number between 1024 and 49151.

Then, on the login node, load the JupyterLab module and start a JupyterLab session as follows:

```
cd RESULTS_FOLDER
module purge && module load JupyterLab
jupyter-lab --port PORT --no-browser
```

where

- `RESULTS_FOLDER` is the folder (on NeSI) where the results you want to inspect are,
- `PORT` is the same number that you choose for your SSH tunnel.

JupyterLab will print on the command line a url looking like:

```
http://localhost:PORT/lab?token=XXXXXXXXXXXXXXXXX
```

Copy and paste it (including the long token string) in your web-browser of choice to access the JupyterLab interface.

*Note: Closing your SSH session or pressing CTRL-C twice in the terminal of your SSH session will terminate the JupyterLab session.*


## TODO

- document `nesi/snakemake.sl` changing `~/.condarc` (in particular channel priority)

- how to unset `conda init` or avoid it to be an issue?

- how to generate a minimal reproducible conda environment (maintainer doc?)

```
module purge
module load Miniconda3/4.12.0
export PYTHONNOUSERSITE=1
conda env create -f envs/mri_base.yaml -p ./mri_env
conda env export -p ./mri_env --no-builds | grep -v '^prefix:' > envs/mri.yaml
conda env remove -p ./mri_env
```
