Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 8
Rules claiming more threads will be scaled down.
Provided resources: mem_mb=512, mem_mib=489, disk_mb=1000, disk_mib=954, cpus=8, time_min=120, gpu=0, mem_gb=30
Select jobs to execute...

[Sun Apr 23 15:30:02 2023]
rule mriqc:
    input: /nesi/nobackup/uoa03264/fMRIpipeline_data/processed_test_cross/bids/sub-ctrl18/ses-a
    output: /nesi/nobackup/uoa03264/fMRIpipeline_data/processed_test_cross/bids/derivatives/mriqc/sub-ctrl18
    log: /nesi/nobackup/uoa03264/fMRIpipeline_data/processed_test_cross/bids/derivatives/fmriprep/sub-ctrl18.html
    jobid: 0
    reason: Missing output files: /nesi/nobackup/uoa03264/fMRIpipeline_data/processed_test_cross/bids/derivatives/mriqc/sub-ctrl18
    wildcards: resultsdir=/nesi/nobackup/uoa03264/fMRIpipeline_data/processed_test_cross, subject=ctrl18
    threads: 8
    resources: mem_mb=512, mem_mib=489, disk_mb=1000, disk_mib=954, tmpdir=/dev/shm/jobs/34739905, cpus=8, time_min=120, gpu=0, mem_gb=30

mriqc /nesi/nobackup/uoa03264/fMRIpipeline_data/processed_test_cross/bids /nesi/nobackup/uoa03264/fMRIpipeline_data/processed_test_cross/bids/derivatives/mriqc participant --participant-label ctrl18 --mem-gb 30 --nprocs 8 --no-sub -w /nesi/nobackup/uoa03264/fMRIpipeline_data/processed_test_cross/work 
Activating singularity image /scale_wlg_persistent/filesets/project/uoa03264/fmri_workflow/.snakemake/singularity/62b788ffbe002df7aabb5b5b0b010147.simg
230423-03:30:09,259 cli IMPORTANT:
	 
    Running MRIQC version 23.0.1:
      * BIDS dataset path: /nesi/nobackup/uoa03264/fMRIpipeline_data/processed_test_cross/bids.
      * Output folder: /nesi/nobackup/uoa03264/fMRIpipeline_data/processed_test_cross/bids/derivatives/mriqc.
      * Analysis levels: ['participant'].

Traceback (most recent call last):
  File "/opt/conda/bin/mriqc", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.9/site-packages/mriqc/cli/run.py", line 167, in main
exception calling callback for <Future at 0x155541c542e0 state=finished raised BrokenProcessPool>
Traceback (most recent call last):
  File "/opt/conda/lib/python3.9/concurrent/futures/_base.py", line 330, in _invoke_callbacks
    callback(self)
  File "/opt/conda/lib/python3.9/site-packages/mriqc/engine/plugin.py", line 451, in _async_callback
    result = args.result()
  File "/opt/conda/lib/python3.9/concurrent/futures/_base.py", line 439, in result
    return self.__get_result()
  File "/opt/conda/lib/python3.9/concurrent/futures/_base.py", line 391, in __get_result
    raise self._exception
concurrent.futures.process.BrokenProcessPool: A process in the process pool was terminated abruptly while the future was running or pending.
    mriqc_wf.run(**_plugin)
  File "/opt/conda/lib/python3.9/site-packages/nipype/pipeline/engine/workflows.py", line 638, in run
exception calling callback for <Future at 0x155541c54d30 state=finished raised BrokenProcessPool>
Traceback (most recent call last):
  File "/opt/conda/lib/python3.9/concurrent/futures/_base.py", line 330, in _invoke_callbacks
    callback(self)
  File "/opt/conda/lib/python3.9/site-packages/mriqc/engine/plugin.py", line 451, in _async_callback
    result = args.result()
  File "/opt/conda/lib/python3.9/concurrent/futures/_base.py", line 439, in result
    return self.__get_result()
  File "/opt/conda/lib/python3.9/concurrent/futures/_base.py", line 391, in __get_result
    raise self._exception
  File "/opt/conda/lib/python3.9/concurrent/futures/_base.py", line 330, in _invoke_callbacks
    callback(self)
  File "/opt/conda/lib/python3.9/site-packages/mriqc/engine/plugin.py", line 451, in _async_callback
    result = args.result()
  File "/opt/conda/lib/python3.9/concurrent/futures/_base.py", line 439, in result
    return self.__get_result()
  File "/opt/conda/lib/python3.9/concurrent/futures/_base.py", line 391, in __get_result
    raise self._exception
concurrent.futures.process.BrokenProcessPool: A process in the process pool was terminated abruptly while the future was running or pending.
    runner.run(execgraph, updatehash=updatehash, config=self.config)
  File "/opt/conda/lib/python3.9/site-packages/mriqc/engine/plugin.py", line 200, in run
    self._send_procs_to_workers(updatehash=updatehash, graph=graph)
  File "/opt/conda/lib/python3.9/site-packages/mriqc/engine/plugin.py", line 599, in _send_procs_to_workers
    tid = self._submit_job(deepcopy(self.procs[jobid]), updatehash=updatehash)
  File "/opt/conda/lib/python3.9/site-packages/mriqc/engine/plugin.py", line 467, in _submit_job
    result_future = self.pool.submit(run_node, node, updatehash, self._taskid)
  File "/opt/conda/lib/python3.9/concurrent/futures/process.py", line 681, in submit
    raise BrokenProcessPool(self._broken)
concurrent.futures.process.BrokenProcessPool: A child process terminated abruptly, the process pool is not usable anymore
[Sun Apr 23 15:30:31 2023]
Error in rule mriqc:
    jobid: 0
    input: /nesi/nobackup/uoa03264/fMRIpipeline_data/processed_test_cross/bids/sub-ctrl18/ses-a
    output: /nesi/nobackup/uoa03264/fMRIpipeline_data/processed_test_cross/bids/derivatives/mriqc/sub-ctrl18
    log: /nesi/nobackup/uoa03264/fMRIpipeline_data/processed_test_cross/bids/derivatives/fmriprep/sub-ctrl18.html (check log file(s) for error details)
    shell:
        mriqc /nesi/nobackup/uoa03264/fMRIpipeline_data/processed_test_cross/bids /nesi/nobackup/uoa03264/fMRIpipeline_data/processed_test_cross/bids/derivatives/mriqc participant --participant-label ctrl18 --mem-gb 30 --nprocs 8 --no-sub -w /nesi/nobackup/uoa03264/fMRIpipeline_data/processed_test_cross/work 
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
slurmstepd: error: Detected 1 oom-kill event(s) in StepId=34739905.batch. Some of your processes may have been killed by the cgroup out-of-memory handler.
