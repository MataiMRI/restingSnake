Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 16
Rules claiming more threads will be scaled down.
Select jobs to execute...

[Thu Nov 17 08:23:33 2022]
rule freesurfer:
    input: /nesi/nobackup/uoa03264/fMRIpipeline_data/processed_test/bids/sub-control17/ses-b
    output: /nesi/nobackup/uoa03264/fMRIpipeline_data/processed_test/bids/derivatives/freesurfer/sub-control17-b
    jobid: 0
    wildcards: resultsdir=/nesi/nobackup/uoa03264/fMRIpipeline_data/processed_test, subject=control17, session=b
    threads: 16
    resources: mem_mb=20000, disk_mb=1000, tmpdir=/dev/shm/jobs/31375904, cpus=16, time_min=600, gpu=0

recon-all -sd /nesi/nobackup/uoa03264/fMRIpipeline_data/processed_test/bids/derivatives/freesurfer -i /nesi/nobackup/uoa03264/fMRIpipeline_data/processed_test/bids/sub-control17/ses-b/anat/sub-control17_ses-b_run-001_T1w.nii.gz -subjid sub-control17 -all -qcache -3T 
Activating singularity image /scale_wlg_persistent/filesets/project/uoa03264/fmri_workflow/.snakemake/singularity/b862ddfcead2bab5771175db9e31b12f.simg
ERROR: You are trying to re-run an existing subject with (possibly)
 new input data (-i). If this is truly new input data, you should delete
 the subject folder and re-run, or specify a different subject name.
 If you are just continuing an analysis of an existing subject, then 
 omit all -i flags.
[Thu Nov 17 08:23:33 2022]
Error in rule freesurfer:
    jobid: 0
    output: /nesi/nobackup/uoa03264/fMRIpipeline_data/processed_test/bids/derivatives/freesurfer/sub-control17-b
    shell:
        recon-all -sd /nesi/nobackup/uoa03264/fMRIpipeline_data/processed_test/bids/derivatives/freesurfer -i /nesi/nobackup/uoa03264/fMRIpipeline_data/processed_test/bids/sub-control17/ses-b/anat/sub-control17_ses-b_run-001_T1w.nii.gz -subjid sub-control17 -all -qcache -3T 
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
