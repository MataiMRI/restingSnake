Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 2
Rules claiming more threads will be scaled down.
Singularity containers: ignored
Job stats:
job          count    min threads    max threads
---------  -------  -------------  -------------
all              1              1              1
heudiconv        3              1              1
total            4              1              1

Select jobs to execute...

[Wed Oct  5 18:41:07 2022]
rule heudiconv:
    input: scan_list.csv
    output: bids/sub-control_17/ses-a/anat/sub-control_17_ses-a_run-001_T1w.nii.gz
    jobid: 4
    wildcards: cohort=control, subject=17, session=a
    resources: tmpdir=/tmp


[Wed Oct  5 18:41:07 2022]
rule heudiconv:
    input: scan_list.csv
    output: bids/sub-control_19/ses-a/anat/sub-control_19_ses-a_run-001_T1w.nii.gz
    jobid: 3
    wildcards: cohort=control, subject=19, session=a
    resources: tmpdir=/tmp

[Wed Oct  5 18:41:43 2022]
Error in rule heudiconv:
    jobid: 3
    output: bids/sub-control_19/ses-a/anat/sub-control_19_ses-a_run-001_T1w.nii.gz
    shell:
         docker run --rm -it -v /home/jpmcgeown/data/fmri:/data -v /home/jpmcgeown/github/fmri_workflow:/base             nipy/heudiconv:latest             -d /data/dicom/sub_{subject}/ses_{session}/*/*             -o /data/bids             -f /base/scripts/heuristic.py             -s control_19             -ss a             -c dcm2niix             -b             --overwrite
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

